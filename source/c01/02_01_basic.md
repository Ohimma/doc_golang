## 2.01 语言基础：计算机

#### 1. 进制转换

```
其他进制转十进制
二进制  1011 = 1*2的0次方 + 1*2的1次方 + 0*2的2次方 + 1*2的3次方 = 11
八进制  0123 = 3*8的0次方 + 2*8的1次方 + 1*8的2次方 + 0*8的3次方 = 83

十进制转其他进制
二进制  将该数不断除以2，直至商为0，并从下往上把余数拼接起来
八进制  将该数不断除以8，直至商为0，并从下往上把余数拼接起来

其他进制转二进制
八进制  将二进制每3位数进行组合，分别对应位数 111 == 421
十进制  将二进制每4位数进行组合，分别对应位数 1111 == 8421

二进制转其他进制
八进制  将每一位数转成3位的组合
十进制  将每一位数转成4位的组合

```

#### 2. 认识编码

**a) 计算机编码解码**  
存储：字符 -> utf-8 -> unicode -> 010101 -> 内存  
获取：内存 -> 010101 -> unicode -> utf-8 -> 字符

**b) ascii 码**  
计算机的存储只认二进制位(bit)字符，即 0 和 1  
8 个 bit 组合成 256 种状态，等于一个字节(byte)，即一个字节有 256 种状态  
上世纪 60 年代，美国制定了 ascii 字符编码，标识字符串和二进制位的关系

**c) unicode**  
世界上存储着多种编码方式，如果发送和接受编码不一样就会乱码。所以统一了一种编码，将所有字符纳入其中，即 unicode 码

unicode 只是一个字符集，但是却没有规定应该如何存储。  
这样就无法区分三个字节表示的是一个 unicode 还是三个 ascii 了？如果 unicode 统一规定存储用三个或四个字节，那就对英文字母的存储太浪费了

**d) utf-8**  
utf-8 是 unicode 的实现方式之一, 也是使用最广的一种实现方式

规则：  
对单字节字符号，字节第一位是 0，后面七位是 unicode 码  
对 n 字节符号，第一个字节的前 n 位都设为 1，汉字 3 个字节，英文一个字节
